{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0c56dc1-45c5-43d2-be7c-fa98b2107023",
   "metadata": {},
   "source": [
    "# IMPORTING and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1421436a-1e09-4f58-b477-a2b1302b2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5dbbe826-4690-494c-8751-0a3db951d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.tensor([\n",
    "    0,\n",
    "    0.05,\n",
    "    0.1,\n",
    "    0.15,\n",
    "    0.2,\n",
    "    0.25,\n",
    "    0.3,\n",
    "    0.35,\n",
    "    0.4,\n",
    "    0.45,\n",
    "    0.5,\n",
    "    0.55,\n",
    "    0.6,\n",
    "    0.65,\n",
    "    0.7,\n",
    "    0.75,\n",
    "    0.8,\n",
    "    0.85,\n",
    "    0.9,\n",
    "    0.95,\n",
    "    1.0,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7bef8c36-2a14-47d6-a336-4bb0e8bf74ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= torch.tensor( [\n",
    "    0,\n",
    "    0.1,\n",
    "    0.2,\n",
    "    0.3,\n",
    "    0.4,\n",
    "    0.5,\n",
    "    0.6,\n",
    "    0.7,\n",
    "    0.8,\n",
    "    0.9,\n",
    "    1,\n",
    "    0.9,\n",
    "    0.8,\n",
    "    0.7,\n",
    "    0.6,\n",
    "    0.5,\n",
    "    0.4,\n",
    "    0.3,\n",
    "    0.2,\n",
    "    0.1,\n",
    "    0,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfaec9f-d3f1-4c5d-be24-cc311bd3e56e",
   "metadata": {},
   "source": [
    "# Reshaping the data as batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2dd3d619-def2-49ff-881e-c8af4e37c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x.reshape(x.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c042d908-b828-4825-be49-d8e2e60763a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y.reshape(y.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "90679487-4dfe-4d5d-bfd6-755af78e515d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000],\n",
       "        [0.0500],\n",
       "        [0.1000],\n",
       "        [0.1500],\n",
       "        [0.2000],\n",
       "        [0.2500],\n",
       "        [0.3000],\n",
       "        [0.3500],\n",
       "        [0.4000],\n",
       "        [0.4500],\n",
       "        [0.5000],\n",
       "        [0.5500],\n",
       "        [0.6000],\n",
       "        [0.6500],\n",
       "        [0.7000],\n",
       "        [0.7500],\n",
       "        [0.8000],\n",
       "        [0.8500],\n",
       "        [0.9000],\n",
       "        [0.9500],\n",
       "        [1.0000]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "37c64739-48aa-42a9-be0f-384adb6f8f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000],\n",
       "        [0.1000],\n",
       "        [0.2000],\n",
       "        [0.3000],\n",
       "        [0.4000],\n",
       "        [0.5000],\n",
       "        [0.6000],\n",
       "        [0.7000],\n",
       "        [0.8000],\n",
       "        [0.9000],\n",
       "        [1.0000],\n",
       "        [0.9000],\n",
       "        [0.8000],\n",
       "        [0.7000],\n",
       "        [0.6000],\n",
       "        [0.5000],\n",
       "        [0.4000],\n",
       "        [0.3000],\n",
       "        [0.2000],\n",
       "        [0.1000],\n",
       "        [0.0000]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c455fa-0034-403a-8c17-2e078a1ccf0f",
   "metadata": {},
   "source": [
    "# For our nx1 data , we have to pass it to 2 neurons and then biases \n",
    "# so 1x2 weigts to get nx2 \n",
    "# then 1x2 biases , a bias for each neuron \n",
    "# so nx2 + 1x2 = nx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "383394eb-625d-43f9-82ea-08a3bd71914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1= torch.rand(1,2, requires_grad=True)\n",
    "b1=torch.rand(1,2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d1b32f88-42af-4b20-906c-adbc0f909655",
   "metadata": {},
   "outputs": [],
   "source": [
    "summation1=x.matmul(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "52be1fe5-41db-4e57-913a-20ba042a790f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000],\n",
       "        [0.0253, 0.0029],\n",
       "        [0.0505, 0.0058],\n",
       "        [0.0758, 0.0086],\n",
       "        [0.1011, 0.0115],\n",
       "        [0.1263, 0.0144],\n",
       "        [0.1516, 0.0173],\n",
       "        [0.1769, 0.0202],\n",
       "        [0.2022, 0.0231],\n",
       "        [0.2274, 0.0259],\n",
       "        [0.2527, 0.0288],\n",
       "        [0.2780, 0.0317],\n",
       "        [0.3032, 0.0346],\n",
       "        [0.3285, 0.0375],\n",
       "        [0.3538, 0.0403],\n",
       "        [0.3790, 0.0432],\n",
       "        [0.4043, 0.0461],\n",
       "        [0.4296, 0.0490],\n",
       "        [0.4548, 0.0519],\n",
       "        [0.4801, 0.0547],\n",
       "        [0.5054, 0.0576]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summation1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03e5f994-7595-42fc-aeeb-5b431ac75344",
   "metadata": {},
   "outputs": [],
   "source": [
    "output1= summation1 + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "252b159e-fa2e-49df-a007-c0ab2384fb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2460, 0.4251],\n",
       "        [0.2713, 0.4280],\n",
       "        [0.2965, 0.4309],\n",
       "        [0.3218, 0.4338],\n",
       "        [0.3471, 0.4367],\n",
       "        [0.3723, 0.4395],\n",
       "        [0.3976, 0.4424],\n",
       "        [0.4229, 0.4453],\n",
       "        [0.4482, 0.4482],\n",
       "        [0.4734, 0.4511],\n",
       "        [0.4987, 0.4540],\n",
       "        [0.5240, 0.4568],\n",
       "        [0.5492, 0.4597],\n",
       "        [0.5745, 0.4626],\n",
       "        [0.5998, 0.4655],\n",
       "        [0.6250, 0.4684],\n",
       "        [0.6503, 0.4712],\n",
       "        [0.6756, 0.4741],\n",
       "        [0.7008, 0.4770],\n",
       "        [0.7261, 0.4799],\n",
       "        [0.7514, 0.4828]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f805ce5-e683-4ab8-8674-ee7d3e5b1643",
   "metadata": {},
   "source": [
    "# Now we have data in the form nx2 ,  the output of 2 neurons , \n",
    "# Now to feed it into a single final neuron multiply it with a weights vector of 2x1 to get nx1 data output \n",
    "# from final neuron and a single bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3e1e520d-2a6b-4d2f-99ce-b7db5dae8648",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2= torch.rand(2,1, requires_grad=True)\n",
    "b2=torch.rand(1,1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "235843fa-a49f-46af-a199-980edc5e40bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "summation2= output1.matmul(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1098e3a4-331a-4cf9-9a3c-1a42f44fb88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2506],\n",
       "        [0.2689],\n",
       "        [0.2871],\n",
       "        [0.3054],\n",
       "        [0.3237],\n",
       "        [0.3419],\n",
       "        [0.3602],\n",
       "        [0.3785],\n",
       "        [0.3967],\n",
       "        [0.4150],\n",
       "        [0.4333],\n",
       "        [0.4515],\n",
       "        [0.4698],\n",
       "        [0.4881],\n",
       "        [0.5063],\n",
       "        [0.5246],\n",
       "        [0.5429],\n",
       "        [0.5611],\n",
       "        [0.5794],\n",
       "        [0.5977],\n",
       "        [0.6159]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summation2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "36d39dba-4fc6-41ba-afcb-61914593af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "output2=summation2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b97fe91a-1969-44d5-b13c-731bc787d20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7045],\n",
       "        [0.7228],\n",
       "        [0.7411],\n",
       "        [0.7593],\n",
       "        [0.7776],\n",
       "        [0.7959],\n",
       "        [0.8141],\n",
       "        [0.8324],\n",
       "        [0.8507],\n",
       "        [0.8690],\n",
       "        [0.8872],\n",
       "        [0.9055],\n",
       "        [0.9238],\n",
       "        [0.9420],\n",
       "        [0.9603],\n",
       "        [0.9786],\n",
       "        [0.9968],\n",
       "        [1.0151],\n",
       "        [1.0334],\n",
       "        [1.0516],\n",
       "        [1.0699]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "defea4c3-e728-40e1-8d56-a9730edc3821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000],\n",
       "        [0.1000],\n",
       "        [0.2000],\n",
       "        [0.3000],\n",
       "        [0.4000],\n",
       "        [0.5000],\n",
       "        [0.6000],\n",
       "        [0.7000],\n",
       "        [0.8000],\n",
       "        [0.9000],\n",
       "        [1.0000],\n",
       "        [0.9000],\n",
       "        [0.8000],\n",
       "        [0.7000],\n",
       "        [0.6000],\n",
       "        [0.5000],\n",
       "        [0.4000],\n",
       "        [0.3000],\n",
       "        [0.2000],\n",
       "        [0.1000],\n",
       "        [0.0000]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5976ca-be7f-4849-a0cf-cde9418d1d25",
   "metadata": {},
   "source": [
    "# Our mean squared error formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5671fd6b-5a8a-4d8f-8290-7b78baeb913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss= ((y- output2)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e64bcb7-688a-4c3e-b000-2ff76c20f615",
   "metadata": {},
   "source": [
    "# Now backpropagate the whole graph with respect to the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f61fffe-6aae-4046-89d2-eeeabcf70703",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8c947b-e564-4ee3-9788-cca67450a7cb",
   "metadata": {},
   "source": [
    "# We have the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d2c740d2-e6a7-41d0-b3a8-67ca0a6e99ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3356, 0.0876]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bf3da536-335f-49bf-9a7d-93e1e9c8a8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4438],\n",
       "        [0.3770]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a6ca74e0-347c-4a49-ba27-71847bc7fbbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5771, 0.1506]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7af46084-4d59-4a15-9261-818076f4c937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8221]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b925c8b3-d00e-4b3a-b748-f9694749ad5b",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034f5fc-8462-43ce-ae77-768719b431ea",
   "metadata": {},
   "source": [
    "# Lets Put it together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207188ea-040a-4b96-a775-fc0d595ce515",
   "metadata": {},
   "source": [
    "## Initialize the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0602f12f-0d7f-46fe-b062-14c10ab56bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1= torch.rand(1,2, requires_grad=True)\n",
    "b1=torch.rand(1,2, requires_grad=True)\n",
    "w2= torch.rand(2,1, requires_grad=True)\n",
    "b2=torch.rand(1,1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7b2f88-97e2-4544-9898-a34935ca661b",
   "metadata": {},
   "source": [
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f1000086-1923-409a-8421-7988591ac607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    summation1=x.matmul(w1)\n",
    "    output1= summation1 + b1\n",
    "    summation2= output1.matmul(w2)\n",
    "    output2=summation2 + b2\n",
    "    return output2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9253720-c023-4f77-9283-4bc9f79eec49",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2b811f0d-ddb4-41f2-a7a0-b788ce16ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y,yPred):\n",
    "    return ((y- yPred)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a3d477c5-abd9-481f-b67b-044327829946",
   "metadata": {},
   "outputs": [],
   "source": [
    "nIter=1000\n",
    "learningRate=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf9d6cf-26cf-47c1-8880-346fef6d9dc8",
   "metadata": {},
   "source": [
    "## For each of epoch, forward pass , calculate the loss , and then backward pass to calculate the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f57ea8-26ff-40a1-86e2-740772502f4e",
   "metadata": {},
   "source": [
    "## Then according to the gradients, update the weights, but weight updation is not the part of computational graph,\n",
    "## so we do that with gradient detachment, otherwise it would get included in the gradient calculation\n",
    "## Then finally we need to clear the gradient variables to stop it from accumulating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ed4d3b2b-fdc5-4993-a520-d1fbcb3ec066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1:  loss = 0.09379084\n",
      "epoch 2:  loss = 0.09378849\n",
      "epoch 3:  loss = 0.09378615\n",
      "epoch 4:  loss = 0.09378382\n",
      "epoch 5:  loss = 0.09378148\n",
      "epoch 6:  loss = 0.09377915\n",
      "epoch 7:  loss = 0.09377683\n",
      "epoch 8:  loss = 0.09377450\n",
      "epoch 9:  loss = 0.09377220\n",
      "epoch 10:  loss = 0.09376989\n",
      "epoch 11:  loss = 0.09376757\n",
      "epoch 12:  loss = 0.09376526\n",
      "epoch 13:  loss = 0.09376297\n",
      "epoch 14:  loss = 0.09376067\n",
      "epoch 15:  loss = 0.09375838\n",
      "epoch 16:  loss = 0.09375609\n",
      "epoch 17:  loss = 0.09375378\n",
      "epoch 18:  loss = 0.09375151\n",
      "epoch 19:  loss = 0.09374923\n",
      "epoch 20:  loss = 0.09374696\n",
      "epoch 21:  loss = 0.09374470\n",
      "epoch 22:  loss = 0.09374242\n",
      "epoch 23:  loss = 0.09374017\n",
      "epoch 24:  loss = 0.09373791\n",
      "epoch 25:  loss = 0.09373564\n",
      "epoch 26:  loss = 0.09373339\n",
      "epoch 27:  loss = 0.09373115\n",
      "epoch 28:  loss = 0.09372891\n",
      "epoch 29:  loss = 0.09372666\n",
      "epoch 30:  loss = 0.09372442\n",
      "epoch 31:  loss = 0.09372219\n",
      "epoch 32:  loss = 0.09371994\n",
      "epoch 33:  loss = 0.09371774\n",
      "epoch 34:  loss = 0.09371551\n",
      "epoch 35:  loss = 0.09371330\n",
      "epoch 36:  loss = 0.09371106\n",
      "epoch 37:  loss = 0.09370886\n",
      "epoch 38:  loss = 0.09370666\n",
      "epoch 39:  loss = 0.09370444\n",
      "epoch 40:  loss = 0.09370224\n",
      "epoch 41:  loss = 0.09370006\n",
      "epoch 42:  loss = 0.09369785\n",
      "epoch 43:  loss = 0.09369566\n",
      "epoch 44:  loss = 0.09369349\n",
      "epoch 45:  loss = 0.09369130\n",
      "epoch 46:  loss = 0.09368913\n",
      "epoch 47:  loss = 0.09368695\n",
      "epoch 48:  loss = 0.09368478\n",
      "epoch 49:  loss = 0.09368261\n",
      "epoch 50:  loss = 0.09368044\n",
      "epoch 51:  loss = 0.09367829\n",
      "epoch 52:  loss = 0.09367613\n",
      "epoch 53:  loss = 0.09367397\n",
      "epoch 54:  loss = 0.09367183\n",
      "epoch 55:  loss = 0.09366968\n",
      "epoch 56:  loss = 0.09366753\n",
      "epoch 57:  loss = 0.09366540\n",
      "epoch 58:  loss = 0.09366325\n",
      "epoch 59:  loss = 0.09366113\n",
      "epoch 60:  loss = 0.09365901\n",
      "epoch 61:  loss = 0.09365688\n",
      "epoch 62:  loss = 0.09365474\n",
      "epoch 63:  loss = 0.09365262\n",
      "epoch 64:  loss = 0.09365051\n",
      "epoch 65:  loss = 0.09364840\n",
      "epoch 66:  loss = 0.09364630\n",
      "epoch 67:  loss = 0.09364418\n",
      "epoch 68:  loss = 0.09364209\n",
      "epoch 69:  loss = 0.09363998\n",
      "epoch 70:  loss = 0.09363788\n",
      "epoch 71:  loss = 0.09363581\n",
      "epoch 72:  loss = 0.09363371\n",
      "epoch 73:  loss = 0.09363163\n",
      "epoch 74:  loss = 0.09362955\n",
      "epoch 75:  loss = 0.09362748\n",
      "epoch 76:  loss = 0.09362540\n",
      "epoch 77:  loss = 0.09362333\n",
      "epoch 78:  loss = 0.09362126\n",
      "epoch 79:  loss = 0.09361918\n",
      "epoch 80:  loss = 0.09361713\n",
      "epoch 81:  loss = 0.09361508\n",
      "epoch 82:  loss = 0.09361304\n",
      "epoch 83:  loss = 0.09361098\n",
      "epoch 84:  loss = 0.09360892\n",
      "epoch 85:  loss = 0.09360687\n",
      "epoch 86:  loss = 0.09360484\n",
      "epoch 87:  loss = 0.09360281\n",
      "epoch 88:  loss = 0.09360076\n",
      "epoch 89:  loss = 0.09359874\n",
      "epoch 90:  loss = 0.09359673\n",
      "epoch 91:  loss = 0.09359469\n",
      "epoch 92:  loss = 0.09359267\n",
      "epoch 93:  loss = 0.09359065\n",
      "epoch 94:  loss = 0.09358865\n",
      "epoch 95:  loss = 0.09358664\n",
      "epoch 96:  loss = 0.09358463\n",
      "epoch 97:  loss = 0.09358262\n",
      "epoch 98:  loss = 0.09358062\n",
      "epoch 99:  loss = 0.09357862\n",
      "epoch 100:  loss = 0.09357663\n",
      "epoch 101:  loss = 0.09357463\n",
      "epoch 102:  loss = 0.09357265\n",
      "epoch 103:  loss = 0.09357066\n",
      "epoch 104:  loss = 0.09356870\n",
      "epoch 105:  loss = 0.09356672\n",
      "epoch 106:  loss = 0.09356473\n",
      "epoch 107:  loss = 0.09356277\n",
      "epoch 108:  loss = 0.09356080\n",
      "epoch 109:  loss = 0.09355883\n",
      "epoch 110:  loss = 0.09355686\n",
      "epoch 111:  loss = 0.09355491\n",
      "epoch 112:  loss = 0.09355295\n",
      "epoch 113:  loss = 0.09355100\n",
      "epoch 114:  loss = 0.09354905\n",
      "epoch 115:  loss = 0.09354711\n",
      "epoch 116:  loss = 0.09354516\n",
      "epoch 117:  loss = 0.09354323\n",
      "epoch 118:  loss = 0.09354128\n",
      "epoch 119:  loss = 0.09353935\n",
      "epoch 120:  loss = 0.09353742\n",
      "epoch 121:  loss = 0.09353550\n",
      "epoch 122:  loss = 0.09353358\n",
      "epoch 123:  loss = 0.09353165\n",
      "epoch 124:  loss = 0.09352974\n",
      "epoch 125:  loss = 0.09352782\n",
      "epoch 126:  loss = 0.09352591\n",
      "epoch 127:  loss = 0.09352401\n",
      "epoch 128:  loss = 0.09352209\n",
      "epoch 129:  loss = 0.09352019\n",
      "epoch 130:  loss = 0.09351829\n",
      "epoch 131:  loss = 0.09351641\n",
      "epoch 132:  loss = 0.09351450\n",
      "epoch 133:  loss = 0.09351262\n",
      "epoch 134:  loss = 0.09351073\n",
      "epoch 135:  loss = 0.09350886\n",
      "epoch 136:  loss = 0.09350697\n",
      "epoch 137:  loss = 0.09350508\n",
      "epoch 138:  loss = 0.09350322\n",
      "epoch 139:  loss = 0.09350134\n",
      "epoch 140:  loss = 0.09349948\n",
      "epoch 141:  loss = 0.09349762\n",
      "epoch 142:  loss = 0.09349575\n",
      "epoch 143:  loss = 0.09349389\n",
      "epoch 144:  loss = 0.09349202\n",
      "epoch 145:  loss = 0.09349018\n",
      "epoch 146:  loss = 0.09348833\n",
      "epoch 147:  loss = 0.09348649\n",
      "epoch 148:  loss = 0.09348464\n",
      "epoch 149:  loss = 0.09348279\n",
      "epoch 150:  loss = 0.09348096\n",
      "epoch 151:  loss = 0.09347913\n",
      "epoch 152:  loss = 0.09347729\n",
      "epoch 153:  loss = 0.09347547\n",
      "epoch 154:  loss = 0.09347364\n",
      "epoch 155:  loss = 0.09347182\n",
      "epoch 156:  loss = 0.09347000\n",
      "epoch 157:  loss = 0.09346817\n",
      "epoch 158:  loss = 0.09346636\n",
      "epoch 159:  loss = 0.09346455\n",
      "epoch 160:  loss = 0.09346274\n",
      "epoch 161:  loss = 0.09346093\n",
      "epoch 162:  loss = 0.09345914\n",
      "epoch 163:  loss = 0.09345733\n",
      "epoch 164:  loss = 0.09345553\n",
      "epoch 165:  loss = 0.09345375\n",
      "epoch 166:  loss = 0.09345194\n",
      "epoch 167:  loss = 0.09345016\n",
      "epoch 168:  loss = 0.09344839\n",
      "epoch 169:  loss = 0.09344660\n",
      "epoch 170:  loss = 0.09344482\n",
      "epoch 171:  loss = 0.09344304\n",
      "epoch 172:  loss = 0.09344126\n",
      "epoch 173:  loss = 0.09343950\n",
      "epoch 174:  loss = 0.09343772\n",
      "epoch 175:  loss = 0.09343596\n",
      "epoch 176:  loss = 0.09343420\n",
      "epoch 177:  loss = 0.09343245\n",
      "epoch 178:  loss = 0.09343068\n",
      "epoch 179:  loss = 0.09342892\n",
      "epoch 180:  loss = 0.09342717\n",
      "epoch 181:  loss = 0.09342543\n",
      "epoch 182:  loss = 0.09342369\n",
      "epoch 183:  loss = 0.09342195\n",
      "epoch 184:  loss = 0.09342019\n",
      "epoch 185:  loss = 0.09341846\n",
      "epoch 186:  loss = 0.09341673\n",
      "epoch 187:  loss = 0.09341501\n",
      "epoch 188:  loss = 0.09341326\n",
      "epoch 189:  loss = 0.09341155\n",
      "epoch 190:  loss = 0.09340981\n",
      "epoch 191:  loss = 0.09340811\n",
      "epoch 192:  loss = 0.09340638\n",
      "epoch 193:  loss = 0.09340468\n",
      "epoch 194:  loss = 0.09340295\n",
      "epoch 195:  loss = 0.09340124\n",
      "epoch 196:  loss = 0.09339955\n",
      "epoch 197:  loss = 0.09339783\n",
      "epoch 198:  loss = 0.09339613\n",
      "epoch 199:  loss = 0.09339444\n",
      "epoch 200:  loss = 0.09339274\n",
      "epoch 201:  loss = 0.09339105\n",
      "epoch 202:  loss = 0.09338935\n",
      "epoch 203:  loss = 0.09338767\n",
      "epoch 204:  loss = 0.09338599\n",
      "epoch 205:  loss = 0.09338430\n",
      "epoch 206:  loss = 0.09338263\n",
      "epoch 207:  loss = 0.09338094\n",
      "epoch 208:  loss = 0.09337927\n",
      "epoch 209:  loss = 0.09337761\n",
      "epoch 210:  loss = 0.09337592\n",
      "epoch 211:  loss = 0.09337427\n",
      "epoch 212:  loss = 0.09337261\n",
      "epoch 213:  loss = 0.09337095\n",
      "epoch 214:  loss = 0.09336928\n",
      "epoch 215:  loss = 0.09336764\n",
      "epoch 216:  loss = 0.09336598\n",
      "epoch 217:  loss = 0.09336434\n",
      "epoch 218:  loss = 0.09336270\n",
      "epoch 219:  loss = 0.09336103\n",
      "epoch 220:  loss = 0.09335940\n",
      "epoch 221:  loss = 0.09335776\n",
      "epoch 222:  loss = 0.09335612\n",
      "epoch 223:  loss = 0.09335449\n",
      "epoch 224:  loss = 0.09335285\n",
      "epoch 225:  loss = 0.09335123\n",
      "epoch 226:  loss = 0.09334961\n",
      "epoch 227:  loss = 0.09334798\n",
      "epoch 228:  loss = 0.09334636\n",
      "epoch 229:  loss = 0.09334473\n",
      "epoch 230:  loss = 0.09334312\n",
      "epoch 231:  loss = 0.09334152\n",
      "epoch 232:  loss = 0.09333991\n",
      "epoch 233:  loss = 0.09333829\n",
      "epoch 234:  loss = 0.09333669\n",
      "epoch 235:  loss = 0.09333509\n",
      "epoch 236:  loss = 0.09333348\n",
      "epoch 237:  loss = 0.09333189\n",
      "epoch 238:  loss = 0.09333029\n",
      "epoch 239:  loss = 0.09332870\n",
      "epoch 240:  loss = 0.09332711\n",
      "epoch 241:  loss = 0.09332552\n",
      "epoch 242:  loss = 0.09332393\n",
      "epoch 243:  loss = 0.09332236\n",
      "epoch 244:  loss = 0.09332079\n",
      "epoch 245:  loss = 0.09331919\n",
      "epoch 246:  loss = 0.09331761\n",
      "epoch 247:  loss = 0.09331605\n",
      "epoch 248:  loss = 0.09331448\n",
      "epoch 249:  loss = 0.09331291\n",
      "epoch 250:  loss = 0.09331135\n",
      "epoch 251:  loss = 0.09330978\n",
      "epoch 252:  loss = 0.09330823\n",
      "epoch 253:  loss = 0.09330667\n",
      "epoch 254:  loss = 0.09330511\n",
      "epoch 255:  loss = 0.09330356\n",
      "epoch 256:  loss = 0.09330201\n",
      "epoch 257:  loss = 0.09330046\n",
      "epoch 258:  loss = 0.09329892\n",
      "epoch 259:  loss = 0.09329737\n",
      "epoch 260:  loss = 0.09329583\n",
      "epoch 261:  loss = 0.09329429\n",
      "epoch 262:  loss = 0.09329277\n",
      "epoch 263:  loss = 0.09329123\n",
      "epoch 264:  loss = 0.09328969\n",
      "epoch 265:  loss = 0.09328817\n",
      "epoch 266:  loss = 0.09328664\n",
      "epoch 267:  loss = 0.09328513\n",
      "epoch 268:  loss = 0.09328360\n",
      "epoch 269:  loss = 0.09328208\n",
      "epoch 270:  loss = 0.09328058\n",
      "epoch 271:  loss = 0.09327906\n",
      "epoch 272:  loss = 0.09327754\n",
      "epoch 273:  loss = 0.09327604\n",
      "epoch 274:  loss = 0.09327454\n",
      "epoch 275:  loss = 0.09327303\n",
      "epoch 276:  loss = 0.09327153\n",
      "epoch 277:  loss = 0.09327003\n",
      "epoch 278:  loss = 0.09326853\n",
      "epoch 279:  loss = 0.09326705\n",
      "epoch 280:  loss = 0.09326554\n",
      "epoch 281:  loss = 0.09326407\n",
      "epoch 282:  loss = 0.09326258\n",
      "epoch 283:  loss = 0.09326109\n",
      "epoch 284:  loss = 0.09325960\n",
      "epoch 285:  loss = 0.09325811\n",
      "epoch 286:  loss = 0.09325664\n",
      "epoch 287:  loss = 0.09325517\n",
      "epoch 288:  loss = 0.09325369\n",
      "epoch 289:  loss = 0.09325223\n",
      "epoch 290:  loss = 0.09325076\n",
      "epoch 291:  loss = 0.09324928\n",
      "epoch 292:  loss = 0.09324783\n",
      "epoch 293:  loss = 0.09324637\n",
      "epoch 294:  loss = 0.09324492\n",
      "epoch 295:  loss = 0.09324346\n",
      "epoch 296:  loss = 0.09324201\n",
      "epoch 297:  loss = 0.09324055\n",
      "epoch 298:  loss = 0.09323911\n",
      "epoch 299:  loss = 0.09323766\n",
      "epoch 300:  loss = 0.09323621\n",
      "epoch 301:  loss = 0.09323476\n",
      "epoch 302:  loss = 0.09323333\n",
      "epoch 303:  loss = 0.09323188\n",
      "epoch 304:  loss = 0.09323045\n",
      "epoch 305:  loss = 0.09322902\n",
      "epoch 306:  loss = 0.09322759\n",
      "epoch 307:  loss = 0.09322616\n",
      "epoch 308:  loss = 0.09322474\n",
      "epoch 309:  loss = 0.09322331\n",
      "epoch 310:  loss = 0.09322190\n",
      "epoch 311:  loss = 0.09322046\n",
      "epoch 312:  loss = 0.09321904\n",
      "epoch 313:  loss = 0.09321763\n",
      "epoch 314:  loss = 0.09321622\n",
      "epoch 315:  loss = 0.09321480\n",
      "epoch 316:  loss = 0.09321340\n",
      "epoch 317:  loss = 0.09321199\n",
      "epoch 318:  loss = 0.09321058\n",
      "epoch 319:  loss = 0.09320918\n",
      "epoch 320:  loss = 0.09320778\n",
      "epoch 321:  loss = 0.09320638\n",
      "epoch 322:  loss = 0.09320499\n",
      "epoch 323:  loss = 0.09320360\n",
      "epoch 324:  loss = 0.09320222\n",
      "epoch 325:  loss = 0.09320082\n",
      "epoch 326:  loss = 0.09319943\n",
      "epoch 327:  loss = 0.09319804\n",
      "epoch 328:  loss = 0.09319667\n",
      "epoch 329:  loss = 0.09319528\n",
      "epoch 330:  loss = 0.09319390\n",
      "epoch 331:  loss = 0.09319254\n",
      "epoch 332:  loss = 0.09319116\n",
      "epoch 333:  loss = 0.09318979\n",
      "epoch 334:  loss = 0.09318842\n",
      "epoch 335:  loss = 0.09318704\n",
      "epoch 336:  loss = 0.09318569\n",
      "epoch 337:  loss = 0.09318432\n",
      "epoch 338:  loss = 0.09318296\n",
      "epoch 339:  loss = 0.09318161\n",
      "epoch 340:  loss = 0.09318024\n",
      "epoch 341:  loss = 0.09317890\n",
      "epoch 342:  loss = 0.09317754\n",
      "epoch 343:  loss = 0.09317619\n",
      "epoch 344:  loss = 0.09317484\n",
      "epoch 345:  loss = 0.09317350\n",
      "epoch 346:  loss = 0.09317216\n",
      "epoch 347:  loss = 0.09317081\n",
      "epoch 348:  loss = 0.09316948\n",
      "epoch 349:  loss = 0.09316814\n",
      "epoch 350:  loss = 0.09316681\n",
      "epoch 351:  loss = 0.09316547\n",
      "epoch 352:  loss = 0.09316415\n",
      "epoch 353:  loss = 0.09316281\n",
      "epoch 354:  loss = 0.09316150\n",
      "epoch 355:  loss = 0.09316017\n",
      "epoch 356:  loss = 0.09315883\n",
      "epoch 357:  loss = 0.09315751\n",
      "epoch 358:  loss = 0.09315620\n",
      "epoch 359:  loss = 0.09315488\n",
      "epoch 360:  loss = 0.09315357\n",
      "epoch 361:  loss = 0.09315225\n",
      "epoch 362:  loss = 0.09315095\n",
      "epoch 363:  loss = 0.09314963\n",
      "epoch 364:  loss = 0.09314834\n",
      "epoch 365:  loss = 0.09314702\n",
      "epoch 366:  loss = 0.09314574\n",
      "epoch 367:  loss = 0.09314442\n",
      "epoch 368:  loss = 0.09314313\n",
      "epoch 369:  loss = 0.09314183\n",
      "epoch 370:  loss = 0.09314054\n",
      "epoch 371:  loss = 0.09313925\n",
      "epoch 372:  loss = 0.09313796\n",
      "epoch 373:  loss = 0.09313666\n",
      "epoch 374:  loss = 0.09313539\n",
      "epoch 375:  loss = 0.09313411\n",
      "epoch 376:  loss = 0.09313282\n",
      "epoch 377:  loss = 0.09313154\n",
      "epoch 378:  loss = 0.09313027\n",
      "epoch 379:  loss = 0.09312899\n",
      "epoch 380:  loss = 0.09312771\n",
      "epoch 381:  loss = 0.09312645\n",
      "epoch 382:  loss = 0.09312517\n",
      "epoch 383:  loss = 0.09312392\n",
      "epoch 384:  loss = 0.09312265\n",
      "epoch 385:  loss = 0.09312139\n",
      "epoch 386:  loss = 0.09312013\n",
      "epoch 387:  loss = 0.09311887\n",
      "epoch 388:  loss = 0.09311761\n",
      "epoch 389:  loss = 0.09311636\n",
      "epoch 390:  loss = 0.09311510\n",
      "epoch 391:  loss = 0.09311385\n",
      "epoch 392:  loss = 0.09311260\n",
      "epoch 393:  loss = 0.09311135\n",
      "epoch 394:  loss = 0.09311010\n",
      "epoch 395:  loss = 0.09310886\n",
      "epoch 396:  loss = 0.09310762\n",
      "epoch 397:  loss = 0.09310639\n",
      "epoch 398:  loss = 0.09310514\n",
      "epoch 399:  loss = 0.09310390\n",
      "epoch 400:  loss = 0.09310267\n",
      "epoch 401:  loss = 0.09310143\n",
      "epoch 402:  loss = 0.09310021\n",
      "epoch 403:  loss = 0.09309898\n",
      "epoch 404:  loss = 0.09309776\n",
      "epoch 405:  loss = 0.09309653\n",
      "epoch 406:  loss = 0.09309532\n",
      "epoch 407:  loss = 0.09309410\n",
      "epoch 408:  loss = 0.09309287\n",
      "epoch 409:  loss = 0.09309165\n",
      "epoch 410:  loss = 0.09309044\n",
      "epoch 411:  loss = 0.09308922\n",
      "epoch 412:  loss = 0.09308802\n",
      "epoch 413:  loss = 0.09308682\n",
      "epoch 414:  loss = 0.09308559\n",
      "epoch 415:  loss = 0.09308439\n",
      "epoch 416:  loss = 0.09308320\n",
      "epoch 417:  loss = 0.09308199\n",
      "epoch 418:  loss = 0.09308078\n",
      "epoch 419:  loss = 0.09307960\n",
      "epoch 420:  loss = 0.09307839\n",
      "epoch 421:  loss = 0.09307721\n",
      "epoch 422:  loss = 0.09307601\n",
      "epoch 423:  loss = 0.09307481\n",
      "epoch 424:  loss = 0.09307363\n",
      "epoch 425:  loss = 0.09307245\n",
      "epoch 426:  loss = 0.09307126\n",
      "epoch 427:  loss = 0.09307007\n",
      "epoch 428:  loss = 0.09306890\n",
      "epoch 429:  loss = 0.09306773\n",
      "epoch 430:  loss = 0.09306654\n",
      "epoch 431:  loss = 0.09306537\n",
      "epoch 432:  loss = 0.09306420\n",
      "epoch 433:  loss = 0.09306303\n",
      "epoch 434:  loss = 0.09306185\n",
      "epoch 435:  loss = 0.09306069\n",
      "epoch 436:  loss = 0.09305952\n",
      "epoch 437:  loss = 0.09305836\n",
      "epoch 438:  loss = 0.09305719\n",
      "epoch 439:  loss = 0.09305604\n",
      "epoch 440:  loss = 0.09305487\n",
      "epoch 441:  loss = 0.09305371\n",
      "epoch 442:  loss = 0.09305256\n",
      "epoch 443:  loss = 0.09305141\n",
      "epoch 444:  loss = 0.09305026\n",
      "epoch 445:  loss = 0.09304911\n",
      "epoch 446:  loss = 0.09304795\n",
      "epoch 447:  loss = 0.09304682\n",
      "epoch 448:  loss = 0.09304567\n",
      "epoch 449:  loss = 0.09304453\n",
      "epoch 450:  loss = 0.09304338\n",
      "epoch 451:  loss = 0.09304224\n",
      "epoch 452:  loss = 0.09304111\n",
      "epoch 453:  loss = 0.09303997\n",
      "epoch 454:  loss = 0.09303883\n",
      "epoch 455:  loss = 0.09303771\n",
      "epoch 456:  loss = 0.09303658\n",
      "epoch 457:  loss = 0.09303544\n",
      "epoch 458:  loss = 0.09303433\n",
      "epoch 459:  loss = 0.09303319\n",
      "epoch 460:  loss = 0.09303208\n",
      "epoch 461:  loss = 0.09303095\n",
      "epoch 462:  loss = 0.09302983\n",
      "epoch 463:  loss = 0.09302872\n",
      "epoch 464:  loss = 0.09302758\n",
      "epoch 465:  loss = 0.09302649\n",
      "epoch 466:  loss = 0.09302536\n",
      "epoch 467:  loss = 0.09302426\n",
      "epoch 468:  loss = 0.09302314\n",
      "epoch 469:  loss = 0.09302205\n",
      "epoch 470:  loss = 0.09302095\n",
      "epoch 471:  loss = 0.09301983\n",
      "epoch 472:  loss = 0.09301873\n",
      "epoch 473:  loss = 0.09301764\n",
      "epoch 474:  loss = 0.09301652\n",
      "epoch 475:  loss = 0.09301542\n",
      "epoch 476:  loss = 0.09301433\n",
      "epoch 477:  loss = 0.09301324\n",
      "epoch 478:  loss = 0.09301215\n",
      "epoch 479:  loss = 0.09301106\n",
      "epoch 480:  loss = 0.09300997\n",
      "epoch 481:  loss = 0.09300889\n",
      "epoch 482:  loss = 0.09300781\n",
      "epoch 483:  loss = 0.09300672\n",
      "epoch 484:  loss = 0.09300563\n",
      "epoch 485:  loss = 0.09300455\n",
      "epoch 486:  loss = 0.09300347\n",
      "epoch 487:  loss = 0.09300239\n",
      "epoch 488:  loss = 0.09300133\n",
      "epoch 489:  loss = 0.09300026\n",
      "epoch 490:  loss = 0.09299918\n",
      "epoch 491:  loss = 0.09299810\n",
      "epoch 492:  loss = 0.09299704\n",
      "epoch 493:  loss = 0.09299598\n",
      "epoch 494:  loss = 0.09299491\n",
      "epoch 495:  loss = 0.09299385\n",
      "epoch 496:  loss = 0.09299278\n",
      "epoch 497:  loss = 0.09299172\n",
      "epoch 498:  loss = 0.09299067\n",
      "epoch 499:  loss = 0.09298962\n",
      "epoch 500:  loss = 0.09298855\n",
      "epoch 501:  loss = 0.09298749\n",
      "epoch 502:  loss = 0.09298644\n",
      "epoch 503:  loss = 0.09298539\n",
      "epoch 504:  loss = 0.09298436\n",
      "epoch 505:  loss = 0.09298330\n",
      "epoch 506:  loss = 0.09298225\n",
      "epoch 507:  loss = 0.09298120\n",
      "epoch 508:  loss = 0.09298016\n",
      "epoch 509:  loss = 0.09297912\n",
      "epoch 510:  loss = 0.09297809\n",
      "epoch 511:  loss = 0.09297704\n",
      "epoch 512:  loss = 0.09297602\n",
      "epoch 513:  loss = 0.09297498\n",
      "epoch 514:  loss = 0.09297395\n",
      "epoch 515:  loss = 0.09297291\n",
      "epoch 516:  loss = 0.09297188\n",
      "epoch 517:  loss = 0.09297086\n",
      "epoch 518:  loss = 0.09296983\n",
      "epoch 519:  loss = 0.09296881\n",
      "epoch 520:  loss = 0.09296779\n",
      "epoch 521:  loss = 0.09296677\n",
      "epoch 522:  loss = 0.09296575\n",
      "epoch 523:  loss = 0.09296472\n",
      "epoch 524:  loss = 0.09296370\n",
      "epoch 525:  loss = 0.09296270\n",
      "epoch 526:  loss = 0.09296168\n",
      "epoch 527:  loss = 0.09296066\n",
      "epoch 528:  loss = 0.09295966\n",
      "epoch 529:  loss = 0.09295865\n",
      "epoch 530:  loss = 0.09295765\n",
      "epoch 531:  loss = 0.09295662\n",
      "epoch 532:  loss = 0.09295564\n",
      "epoch 533:  loss = 0.09295463\n",
      "epoch 534:  loss = 0.09295362\n",
      "epoch 535:  loss = 0.09295262\n",
      "epoch 536:  loss = 0.09295162\n",
      "epoch 537:  loss = 0.09295064\n",
      "epoch 538:  loss = 0.09294964\n",
      "epoch 539:  loss = 0.09294865\n",
      "epoch 540:  loss = 0.09294766\n",
      "epoch 541:  loss = 0.09294667\n",
      "epoch 542:  loss = 0.09294568\n",
      "epoch 543:  loss = 0.09294469\n",
      "epoch 544:  loss = 0.09294371\n",
      "epoch 545:  loss = 0.09294272\n",
      "epoch 546:  loss = 0.09294174\n",
      "epoch 547:  loss = 0.09294076\n",
      "epoch 548:  loss = 0.09293977\n",
      "epoch 549:  loss = 0.09293880\n",
      "epoch 550:  loss = 0.09293782\n",
      "epoch 551:  loss = 0.09293685\n",
      "epoch 552:  loss = 0.09293588\n",
      "epoch 553:  loss = 0.09293490\n",
      "epoch 554:  loss = 0.09293393\n",
      "epoch 555:  loss = 0.09293295\n",
      "epoch 556:  loss = 0.09293200\n",
      "epoch 557:  loss = 0.09293102\n",
      "epoch 558:  loss = 0.09293006\n",
      "epoch 559:  loss = 0.09292910\n",
      "epoch 560:  loss = 0.09292813\n",
      "epoch 561:  loss = 0.09292718\n",
      "epoch 562:  loss = 0.09292622\n",
      "epoch 563:  loss = 0.09292527\n",
      "epoch 564:  loss = 0.09292430\n",
      "epoch 565:  loss = 0.09292336\n",
      "epoch 566:  loss = 0.09292240\n",
      "epoch 567:  loss = 0.09292144\n",
      "epoch 568:  loss = 0.09292050\n",
      "epoch 569:  loss = 0.09291956\n",
      "epoch 570:  loss = 0.09291860\n",
      "epoch 571:  loss = 0.09291766\n",
      "epoch 572:  loss = 0.09291672\n",
      "epoch 573:  loss = 0.09291577\n",
      "epoch 574:  loss = 0.09291483\n",
      "epoch 575:  loss = 0.09291389\n",
      "epoch 576:  loss = 0.09291295\n",
      "epoch 577:  loss = 0.09291202\n",
      "epoch 578:  loss = 0.09291108\n",
      "epoch 579:  loss = 0.09291016\n",
      "epoch 580:  loss = 0.09290922\n",
      "epoch 581:  loss = 0.09290829\n",
      "epoch 582:  loss = 0.09290735\n",
      "epoch 583:  loss = 0.09290642\n",
      "epoch 584:  loss = 0.09290550\n",
      "epoch 585:  loss = 0.09290458\n",
      "epoch 586:  loss = 0.09290365\n",
      "epoch 587:  loss = 0.09290273\n",
      "epoch 588:  loss = 0.09290181\n",
      "epoch 589:  loss = 0.09290089\n",
      "epoch 590:  loss = 0.09289998\n",
      "epoch 591:  loss = 0.09289905\n",
      "epoch 592:  loss = 0.09289815\n",
      "epoch 593:  loss = 0.09289723\n",
      "epoch 594:  loss = 0.09289631\n",
      "epoch 595:  loss = 0.09289541\n",
      "epoch 596:  loss = 0.09289449\n",
      "epoch 597:  loss = 0.09289359\n",
      "epoch 598:  loss = 0.09289268\n",
      "epoch 599:  loss = 0.09289178\n",
      "epoch 600:  loss = 0.09289087\n",
      "epoch 601:  loss = 0.09288996\n",
      "epoch 602:  loss = 0.09288907\n",
      "epoch 603:  loss = 0.09288818\n",
      "epoch 604:  loss = 0.09288727\n",
      "epoch 605:  loss = 0.09288637\n",
      "epoch 606:  loss = 0.09288548\n",
      "epoch 607:  loss = 0.09288458\n",
      "epoch 608:  loss = 0.09288369\n",
      "epoch 609:  loss = 0.09288279\n",
      "epoch 610:  loss = 0.09288190\n",
      "epoch 611:  loss = 0.09288102\n",
      "epoch 612:  loss = 0.09288014\n",
      "epoch 613:  loss = 0.09287924\n",
      "epoch 614:  loss = 0.09287836\n",
      "epoch 615:  loss = 0.09287748\n",
      "epoch 616:  loss = 0.09287661\n",
      "epoch 617:  loss = 0.09287572\n",
      "epoch 618:  loss = 0.09287485\n",
      "epoch 619:  loss = 0.09287395\n",
      "epoch 620:  loss = 0.09287308\n",
      "epoch 621:  loss = 0.09287220\n",
      "epoch 622:  loss = 0.09287133\n",
      "epoch 623:  loss = 0.09287045\n",
      "epoch 624:  loss = 0.09286958\n",
      "epoch 625:  loss = 0.09286872\n",
      "epoch 626:  loss = 0.09286784\n",
      "epoch 627:  loss = 0.09286698\n",
      "epoch 628:  loss = 0.09286612\n",
      "epoch 629:  loss = 0.09286526\n",
      "epoch 630:  loss = 0.09286439\n",
      "epoch 631:  loss = 0.09286352\n",
      "epoch 632:  loss = 0.09286267\n",
      "epoch 633:  loss = 0.09286181\n",
      "epoch 634:  loss = 0.09286095\n",
      "epoch 635:  loss = 0.09286010\n",
      "epoch 636:  loss = 0.09285924\n",
      "epoch 637:  loss = 0.09285838\n",
      "epoch 638:  loss = 0.09285754\n",
      "epoch 639:  loss = 0.09285669\n",
      "epoch 640:  loss = 0.09285583\n",
      "epoch 641:  loss = 0.09285499\n",
      "epoch 642:  loss = 0.09285413\n",
      "epoch 643:  loss = 0.09285329\n",
      "epoch 644:  loss = 0.09285244\n",
      "epoch 645:  loss = 0.09285160\n",
      "epoch 646:  loss = 0.09285076\n",
      "epoch 647:  loss = 0.09284991\n",
      "epoch 648:  loss = 0.09284909\n",
      "epoch 649:  loss = 0.09284825\n",
      "epoch 650:  loss = 0.09284741\n",
      "epoch 651:  loss = 0.09284656\n",
      "epoch 652:  loss = 0.09284574\n",
      "epoch 653:  loss = 0.09284490\n",
      "epoch 654:  loss = 0.09284407\n",
      "epoch 655:  loss = 0.09284323\n",
      "epoch 656:  loss = 0.09284241\n",
      "epoch 657:  loss = 0.09284158\n",
      "epoch 658:  loss = 0.09284075\n",
      "epoch 659:  loss = 0.09283993\n",
      "epoch 660:  loss = 0.09283911\n",
      "epoch 661:  loss = 0.09283827\n",
      "epoch 662:  loss = 0.09283747\n",
      "epoch 663:  loss = 0.09283663\n",
      "epoch 664:  loss = 0.09283582\n",
      "epoch 665:  loss = 0.09283500\n",
      "epoch 666:  loss = 0.09283420\n",
      "epoch 667:  loss = 0.09283336\n",
      "epoch 668:  loss = 0.09283255\n",
      "epoch 669:  loss = 0.09283175\n",
      "epoch 670:  loss = 0.09283094\n",
      "epoch 671:  loss = 0.09283012\n",
      "epoch 672:  loss = 0.09282931\n",
      "epoch 673:  loss = 0.09282851\n",
      "epoch 674:  loss = 0.09282771\n",
      "epoch 675:  loss = 0.09282690\n",
      "epoch 676:  loss = 0.09282608\n",
      "epoch 677:  loss = 0.09282528\n",
      "epoch 678:  loss = 0.09282450\n",
      "epoch 679:  loss = 0.09282368\n",
      "epoch 680:  loss = 0.09282290\n",
      "epoch 681:  loss = 0.09282210\n",
      "epoch 682:  loss = 0.09282129\n",
      "epoch 683:  loss = 0.09282049\n",
      "epoch 684:  loss = 0.09281971\n",
      "epoch 685:  loss = 0.09281891\n",
      "epoch 686:  loss = 0.09281810\n",
      "epoch 687:  loss = 0.09281731\n",
      "epoch 688:  loss = 0.09281654\n",
      "epoch 689:  loss = 0.09281574\n",
      "epoch 690:  loss = 0.09281496\n",
      "epoch 691:  loss = 0.09281417\n",
      "epoch 692:  loss = 0.09281339\n",
      "epoch 693:  loss = 0.09281261\n",
      "epoch 694:  loss = 0.09281183\n",
      "epoch 695:  loss = 0.09281104\n",
      "epoch 696:  loss = 0.09281026\n",
      "epoch 697:  loss = 0.09280949\n",
      "epoch 698:  loss = 0.09280871\n",
      "epoch 699:  loss = 0.09280793\n",
      "epoch 700:  loss = 0.09280715\n",
      "epoch 701:  loss = 0.09280638\n",
      "epoch 702:  loss = 0.09280560\n",
      "epoch 703:  loss = 0.09280483\n",
      "epoch 704:  loss = 0.09280406\n",
      "epoch 705:  loss = 0.09280331\n",
      "epoch 706:  loss = 0.09280252\n",
      "epoch 707:  loss = 0.09280176\n",
      "epoch 708:  loss = 0.09280100\n",
      "epoch 709:  loss = 0.09280022\n",
      "epoch 710:  loss = 0.09279946\n",
      "epoch 711:  loss = 0.09279869\n",
      "epoch 712:  loss = 0.09279794\n",
      "epoch 713:  loss = 0.09279719\n",
      "epoch 714:  loss = 0.09279642\n",
      "epoch 715:  loss = 0.09279566\n",
      "epoch 716:  loss = 0.09279491\n",
      "epoch 717:  loss = 0.09279416\n",
      "epoch 718:  loss = 0.09279340\n",
      "epoch 719:  loss = 0.09279265\n",
      "epoch 720:  loss = 0.09279190\n",
      "epoch 721:  loss = 0.09279114\n",
      "epoch 722:  loss = 0.09279040\n",
      "epoch 723:  loss = 0.09278966\n",
      "epoch 724:  loss = 0.09278890\n",
      "epoch 725:  loss = 0.09278816\n",
      "epoch 726:  loss = 0.09278740\n",
      "epoch 727:  loss = 0.09278666\n",
      "epoch 728:  loss = 0.09278592\n",
      "epoch 729:  loss = 0.09278519\n",
      "epoch 730:  loss = 0.09278444\n",
      "epoch 731:  loss = 0.09278370\n",
      "epoch 732:  loss = 0.09278296\n",
      "epoch 733:  loss = 0.09278222\n",
      "epoch 734:  loss = 0.09278149\n",
      "epoch 735:  loss = 0.09278076\n",
      "epoch 736:  loss = 0.09278002\n",
      "epoch 737:  loss = 0.09277929\n",
      "epoch 738:  loss = 0.09277857\n",
      "epoch 739:  loss = 0.09277783\n",
      "epoch 740:  loss = 0.09277710\n",
      "epoch 741:  loss = 0.09277637\n",
      "epoch 742:  loss = 0.09277564\n",
      "epoch 743:  loss = 0.09277492\n",
      "epoch 744:  loss = 0.09277420\n",
      "epoch 745:  loss = 0.09277347\n",
      "epoch 746:  loss = 0.09277274\n",
      "epoch 747:  loss = 0.09277204\n",
      "epoch 748:  loss = 0.09277131\n",
      "epoch 749:  loss = 0.09277058\n",
      "epoch 750:  loss = 0.09276986\n",
      "epoch 751:  loss = 0.09276915\n",
      "epoch 752:  loss = 0.09276843\n",
      "epoch 753:  loss = 0.09276773\n",
      "epoch 754:  loss = 0.09276700\n",
      "epoch 755:  loss = 0.09276630\n",
      "epoch 756:  loss = 0.09276558\n",
      "epoch 757:  loss = 0.09276487\n",
      "epoch 758:  loss = 0.09276416\n",
      "epoch 759:  loss = 0.09276346\n",
      "epoch 760:  loss = 0.09276275\n",
      "epoch 761:  loss = 0.09276205\n",
      "epoch 762:  loss = 0.09276134\n",
      "epoch 763:  loss = 0.09276064\n",
      "epoch 764:  loss = 0.09275993\n",
      "epoch 765:  loss = 0.09275922\n",
      "epoch 766:  loss = 0.09275853\n",
      "epoch 767:  loss = 0.09275784\n",
      "epoch 768:  loss = 0.09275713\n",
      "epoch 769:  loss = 0.09275643\n",
      "epoch 770:  loss = 0.09275573\n",
      "epoch 771:  loss = 0.09275504\n",
      "epoch 772:  loss = 0.09275435\n",
      "epoch 773:  loss = 0.09275365\n",
      "epoch 774:  loss = 0.09275296\n",
      "epoch 775:  loss = 0.09275226\n",
      "epoch 776:  loss = 0.09275158\n",
      "epoch 777:  loss = 0.09275088\n",
      "epoch 778:  loss = 0.09275020\n",
      "epoch 779:  loss = 0.09274952\n",
      "epoch 780:  loss = 0.09274883\n",
      "epoch 781:  loss = 0.09274815\n",
      "epoch 782:  loss = 0.09274746\n",
      "epoch 783:  loss = 0.09274677\n",
      "epoch 784:  loss = 0.09274609\n",
      "epoch 785:  loss = 0.09274542\n",
      "epoch 786:  loss = 0.09274473\n",
      "epoch 787:  loss = 0.09274407\n",
      "epoch 788:  loss = 0.09274337\n",
      "epoch 789:  loss = 0.09274270\n",
      "epoch 790:  loss = 0.09274203\n",
      "epoch 791:  loss = 0.09274135\n",
      "epoch 792:  loss = 0.09274067\n",
      "epoch 793:  loss = 0.09274000\n",
      "epoch 794:  loss = 0.09273933\n",
      "epoch 795:  loss = 0.09273866\n",
      "epoch 796:  loss = 0.09273799\n",
      "epoch 797:  loss = 0.09273732\n",
      "epoch 798:  loss = 0.09273667\n",
      "epoch 799:  loss = 0.09273598\n",
      "epoch 800:  loss = 0.09273532\n",
      "epoch 801:  loss = 0.09273466\n",
      "epoch 802:  loss = 0.09273399\n",
      "epoch 803:  loss = 0.09273334\n",
      "epoch 804:  loss = 0.09273267\n",
      "epoch 805:  loss = 0.09273201\n",
      "epoch 806:  loss = 0.09273135\n",
      "epoch 807:  loss = 0.09273069\n",
      "epoch 808:  loss = 0.09273003\n",
      "epoch 809:  loss = 0.09272937\n",
      "epoch 810:  loss = 0.09272873\n",
      "epoch 811:  loss = 0.09272808\n",
      "epoch 812:  loss = 0.09272742\n",
      "epoch 813:  loss = 0.09272676\n",
      "epoch 814:  loss = 0.09272610\n",
      "epoch 815:  loss = 0.09272546\n",
      "epoch 816:  loss = 0.09272481\n",
      "epoch 817:  loss = 0.09272417\n",
      "epoch 818:  loss = 0.09272352\n",
      "epoch 819:  loss = 0.09272287\n",
      "epoch 820:  loss = 0.09272221\n",
      "epoch 821:  loss = 0.09272157\n",
      "epoch 822:  loss = 0.09272093\n",
      "epoch 823:  loss = 0.09272030\n",
      "epoch 824:  loss = 0.09271965\n",
      "epoch 825:  loss = 0.09271902\n",
      "epoch 826:  loss = 0.09271838\n",
      "epoch 827:  loss = 0.09271773\n",
      "epoch 828:  loss = 0.09271709\n",
      "epoch 829:  loss = 0.09271646\n",
      "epoch 830:  loss = 0.09271583\n",
      "epoch 831:  loss = 0.09271519\n",
      "epoch 832:  loss = 0.09271456\n",
      "epoch 833:  loss = 0.09271392\n",
      "epoch 834:  loss = 0.09271329\n",
      "epoch 835:  loss = 0.09271266\n",
      "epoch 836:  loss = 0.09271203\n",
      "epoch 837:  loss = 0.09271140\n",
      "epoch 838:  loss = 0.09271077\n",
      "epoch 839:  loss = 0.09271015\n",
      "epoch 840:  loss = 0.09270951\n",
      "epoch 841:  loss = 0.09270889\n",
      "epoch 842:  loss = 0.09270827\n",
      "epoch 843:  loss = 0.09270765\n",
      "epoch 844:  loss = 0.09270702\n",
      "epoch 845:  loss = 0.09270640\n",
      "epoch 846:  loss = 0.09270579\n",
      "epoch 847:  loss = 0.09270517\n",
      "epoch 848:  loss = 0.09270454\n",
      "epoch 849:  loss = 0.09270392\n",
      "epoch 850:  loss = 0.09270331\n",
      "epoch 851:  loss = 0.09270270\n",
      "epoch 852:  loss = 0.09270208\n",
      "epoch 853:  loss = 0.09270146\n",
      "epoch 854:  loss = 0.09270085\n",
      "epoch 855:  loss = 0.09270023\n",
      "epoch 856:  loss = 0.09269962\n",
      "epoch 857:  loss = 0.09269902\n",
      "epoch 858:  loss = 0.09269842\n",
      "epoch 859:  loss = 0.09269780\n",
      "epoch 860:  loss = 0.09269720\n",
      "epoch 861:  loss = 0.09269659\n",
      "epoch 862:  loss = 0.09269599\n",
      "epoch 863:  loss = 0.09269538\n",
      "epoch 864:  loss = 0.09269477\n",
      "epoch 865:  loss = 0.09269418\n",
      "epoch 866:  loss = 0.09269357\n",
      "epoch 867:  loss = 0.09269297\n",
      "epoch 868:  loss = 0.09269236\n",
      "epoch 869:  loss = 0.09269176\n",
      "epoch 870:  loss = 0.09269116\n",
      "epoch 871:  loss = 0.09269057\n",
      "epoch 872:  loss = 0.09268998\n",
      "epoch 873:  loss = 0.09268937\n",
      "epoch 874:  loss = 0.09268878\n",
      "epoch 875:  loss = 0.09268819\n",
      "epoch 876:  loss = 0.09268759\n",
      "epoch 877:  loss = 0.09268701\n",
      "epoch 878:  loss = 0.09268641\n",
      "epoch 879:  loss = 0.09268583\n",
      "epoch 880:  loss = 0.09268524\n",
      "epoch 881:  loss = 0.09268464\n",
      "epoch 882:  loss = 0.09268405\n",
      "epoch 883:  loss = 0.09268347\n",
      "epoch 884:  loss = 0.09268288\n",
      "epoch 885:  loss = 0.09268230\n",
      "epoch 886:  loss = 0.09268171\n",
      "epoch 887:  loss = 0.09268113\n",
      "epoch 888:  loss = 0.09268055\n",
      "epoch 889:  loss = 0.09267997\n",
      "epoch 890:  loss = 0.09267940\n",
      "epoch 891:  loss = 0.09267882\n",
      "epoch 892:  loss = 0.09267823\n",
      "epoch 893:  loss = 0.09267765\n",
      "epoch 894:  loss = 0.09267708\n",
      "epoch 895:  loss = 0.09267650\n",
      "epoch 896:  loss = 0.09267592\n",
      "epoch 897:  loss = 0.09267536\n",
      "epoch 898:  loss = 0.09267477\n",
      "epoch 899:  loss = 0.09267421\n",
      "epoch 900:  loss = 0.09267363\n",
      "epoch 901:  loss = 0.09267306\n",
      "epoch 902:  loss = 0.09267249\n",
      "epoch 903:  loss = 0.09267193\n",
      "epoch 904:  loss = 0.09267136\n",
      "epoch 905:  loss = 0.09267079\n",
      "epoch 906:  loss = 0.09267022\n",
      "epoch 907:  loss = 0.09266965\n",
      "epoch 908:  loss = 0.09266909\n",
      "epoch 909:  loss = 0.09266852\n",
      "epoch 910:  loss = 0.09266795\n",
      "epoch 911:  loss = 0.09266740\n",
      "epoch 912:  loss = 0.09266684\n",
      "epoch 913:  loss = 0.09266628\n",
      "epoch 914:  loss = 0.09266572\n",
      "epoch 915:  loss = 0.09266517\n",
      "epoch 916:  loss = 0.09266461\n",
      "epoch 917:  loss = 0.09266405\n",
      "epoch 918:  loss = 0.09266349\n",
      "epoch 919:  loss = 0.09266293\n",
      "epoch 920:  loss = 0.09266238\n",
      "epoch 921:  loss = 0.09266182\n",
      "epoch 922:  loss = 0.09266126\n",
      "epoch 923:  loss = 0.09266073\n",
      "epoch 924:  loss = 0.09266017\n",
      "epoch 925:  loss = 0.09265961\n",
      "epoch 926:  loss = 0.09265907\n",
      "epoch 927:  loss = 0.09265851\n",
      "epoch 928:  loss = 0.09265797\n",
      "epoch 929:  loss = 0.09265742\n",
      "epoch 930:  loss = 0.09265688\n",
      "epoch 931:  loss = 0.09265634\n",
      "epoch 932:  loss = 0.09265579\n",
      "epoch 933:  loss = 0.09265524\n",
      "epoch 934:  loss = 0.09265470\n",
      "epoch 935:  loss = 0.09265416\n",
      "epoch 936:  loss = 0.09265362\n",
      "epoch 937:  loss = 0.09265308\n",
      "epoch 938:  loss = 0.09265254\n",
      "epoch 939:  loss = 0.09265200\n",
      "epoch 940:  loss = 0.09265145\n",
      "epoch 941:  loss = 0.09265092\n",
      "epoch 942:  loss = 0.09265038\n",
      "epoch 943:  loss = 0.09264985\n",
      "epoch 944:  loss = 0.09264931\n",
      "epoch 945:  loss = 0.09264878\n",
      "epoch 946:  loss = 0.09264825\n",
      "epoch 947:  loss = 0.09264771\n",
      "epoch 948:  loss = 0.09264718\n",
      "epoch 949:  loss = 0.09264664\n",
      "epoch 950:  loss = 0.09264612\n",
      "epoch 951:  loss = 0.09264559\n",
      "epoch 952:  loss = 0.09264506\n",
      "epoch 953:  loss = 0.09264453\n",
      "epoch 954:  loss = 0.09264400\n",
      "epoch 955:  loss = 0.09264348\n",
      "epoch 956:  loss = 0.09264296\n",
      "epoch 957:  loss = 0.09264244\n",
      "epoch 958:  loss = 0.09264191\n",
      "epoch 959:  loss = 0.09264139\n",
      "epoch 960:  loss = 0.09264086\n",
      "epoch 961:  loss = 0.09264034\n",
      "epoch 962:  loss = 0.09263982\n",
      "epoch 963:  loss = 0.09263931\n",
      "epoch 964:  loss = 0.09263879\n",
      "epoch 965:  loss = 0.09263827\n",
      "epoch 966:  loss = 0.09263775\n",
      "epoch 967:  loss = 0.09263724\n",
      "epoch 968:  loss = 0.09263671\n",
      "epoch 969:  loss = 0.09263621\n",
      "epoch 970:  loss = 0.09263569\n",
      "epoch 971:  loss = 0.09263518\n",
      "epoch 972:  loss = 0.09263466\n",
      "epoch 973:  loss = 0.09263414\n",
      "epoch 974:  loss = 0.09263363\n",
      "epoch 975:  loss = 0.09263311\n",
      "epoch 976:  loss = 0.09263262\n",
      "epoch 977:  loss = 0.09263210\n",
      "epoch 978:  loss = 0.09263160\n",
      "epoch 979:  loss = 0.09263109\n",
      "epoch 980:  loss = 0.09263059\n",
      "epoch 981:  loss = 0.09263007\n",
      "epoch 982:  loss = 0.09262957\n",
      "epoch 983:  loss = 0.09262907\n",
      "epoch 984:  loss = 0.09262857\n",
      "epoch 985:  loss = 0.09262806\n",
      "epoch 986:  loss = 0.09262756\n",
      "epoch 987:  loss = 0.09262706\n",
      "epoch 988:  loss = 0.09262656\n",
      "epoch 989:  loss = 0.09262607\n",
      "epoch 990:  loss = 0.09262556\n",
      "epoch 991:  loss = 0.09262506\n",
      "epoch 992:  loss = 0.09262456\n",
      "epoch 993:  loss = 0.09262407\n",
      "epoch 994:  loss = 0.09262357\n",
      "epoch 995:  loss = 0.09262308\n",
      "epoch 996:  loss = 0.09262258\n",
      "epoch 997:  loss = 0.09262209\n",
      "epoch 998:  loss = 0.09262160\n",
      "epoch 999:  loss = 0.09262110\n",
      "epoch 1000:  loss = 0.09262061\n",
      "tensor([[0.4660, 0.7497]], requires_grad=True) tensor([[ 0.1413],\n",
      "        [-0.0086]], requires_grad=True) tensor([[0.3901, 0.4026]], requires_grad=True) tensor([[0.3923]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nIter):\n",
    "    # predict = forward pass\n",
    "    yPred = forward(x)\n",
    "\n",
    "    # loss\n",
    "    l = loss(y, yPred)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    #w.data = w.data - learning_rate * w.grad\n",
    "    with torch.no_grad():\n",
    "        w1 -= learningRate * w1.grad\n",
    "    with torch.no_grad():    \n",
    "        w2 -= learningRate *w2.grad\n",
    "    with torch.no_grad():\n",
    "        b1 -= learningRate * b1.grad\n",
    "    with torch.no_grad():\n",
    "        b2 -= learningRate *b2.grad\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()\n",
    "    b1.grad.zero_()\n",
    "    b2.grad.zero_()\n",
    "\n",
    "    print(f'epoch {epoch+1}:  loss = {l.item():.8f}')\n",
    "print(w1,w2,b1,b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aec63bec-aec8-4306-92e3-d70540d2731f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4660, 0.7497]], requires_grad=True)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a89829-89d1-4c6d-adb8-3a5784940b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
